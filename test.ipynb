{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML_FILES_Size:  0\n",
      "IMG_FILES_Size:  0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m IMG_FILES \u001b[39m=\u001b[39m glob(CFG\u001b[39m.\u001b[39mimg_path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/*.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m XML_FILES \u001b[39m=\u001b[39m glob(CFG\u001b[39m.\u001b[39mxml_path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/*.xml\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m df, classes \u001b[39m=\u001b[39m build_df(XML_FILES)\n\u001b[1;32m     69\u001b[0m data \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m     71\u001b[0m \u001b[39m# input and target\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/myfolder/desktop/fall2022/Vision and Image/Parasitic-Egg-Detection-and-Classification/read_dataset.py:100\u001b[0m, in \u001b[0;36mbuild_df\u001b[0;34m(xml_files)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_df\u001b[39m(xml_files):\n\u001b[0;32m--> 100\u001b[0m     df \u001b[39m=\u001b[39m xml_files_to_df(xml_files)\n\u001b[1;32m    103\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique())\n\u001b[1;32m    104\u001b[0m     cls2id \u001b[39m=\u001b[39m {cls_name: i \u001b[39mfor\u001b[39;00m i, cls_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classes)}\n",
      "File \u001b[0;32m~/Desktop/myfolder/desktop/fall2022/Vision and Image/Parasitic-Egg-Detection-and-Classification/read_dataset.py:86\u001b[0m, in \u001b[0;36mxml_files_to_df\u001b[0;34m(xml_files)\u001b[0m\n\u001b[1;32m     83\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mxmax\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     84\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mymax\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> 86\u001b[0m df[[\u001b[39m'\u001b[39m\u001b[39mxmin\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mymin\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mxmax\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mymax\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mstack([df[\u001b[39m'\u001b[39;49m\u001b[39mboxes\u001b[39;49m\u001b[39m'\u001b[39;49m][i] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(df[\u001b[39m'\u001b[39;49m\u001b[39mboxes\u001b[39;49m\u001b[39m'\u001b[39;49m]))])\n\u001b[1;32m     88\u001b[0m df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mxmin\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mxmin\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/myfolder/desktop/fall2022/torch13/lib/python3.9/site-packages/numpy/core/shape_base.py:422\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    420\u001b[0m arrays \u001b[39m=\u001b[39m [asanyarray(arr) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m arrays:\n\u001b[0;32m--> 422\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mneed at least one array to stack\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    424\u001b[0m shapes \u001b[39m=\u001b[39m {arr\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays}\n\u001b[1;32m    425\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shapes) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from read_dataset import build_df\n",
    "from utils import CFG\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from models import *\n",
    "\n",
    "# Parameters\n",
    "params = {\"batch_size\": 64, \"shuffle\": True, \"num_workers\": 6}\n",
    "max_epochs = 100\n",
    "NUM_CLASSES = 11\n",
    "RESNET_OUT_FEATURES = 1000\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    # x = img_path\n",
    "    # y = 'xmin', 'ymin', 'xmax', 'ymax', 'label'\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        \"Initialization\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        X = self.x[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def split_dataset(train, target, validation=False):\n",
    "    # 70%-20%-10% split, as we're splitting 10% from the already split X_train so we're actually ending up with a 72%-20%-8% split here:\n",
    "    # train = train.to_numpy()\n",
    "    # target = target.to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train, target, train_size=0.8\n",
    "    )\n",
    "\n",
    "    if validation:\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_train, y_train, train_size=0.9\n",
    "        )\n",
    "        train_data = [np.squeeze(X_train, axis=1), y_train]\n",
    "        validation_data = [np.squeeze(X_train, axis=1), y_valid]\n",
    "\n",
    "        return train_data, validation_data, test_data\n",
    "\n",
    "    train_data = [np.squeeze(X_train, axis=1), y_train]\n",
    "    test_data = [np.squeeze(X_train, axis=1), y_test]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ['id', 'label', 'xmin', 'ymin', 'xmax', 'ymax', 'img_path']\n",
    "\n",
    "    IMG_FILES = glob(CFG.img_path + \"/*.jpg\")\n",
    "    XML_FILES = glob(CFG.xml_path + \"/*.xml\")\n",
    "    df, classes = build_df(XML_FILES)\n",
    "    data = df.to_numpy()\n",
    "\n",
    "    # input and target\n",
    "    input = df[[\"img_path\"]]\n",
    "    target = df[[\"xmin\", \"ymin\", \"xmax\", \"ymax\", \"label\"]]\n",
    "    # print(\"target: \",(target.shape))\n",
    "\n",
    "    # splitting data\n",
    "    # train_data, validation_data, test_data = split_dataset(input,target,True)\n",
    "    train_data, test_data = split_dataset(input, target, validation=False)\n",
    "\n",
    "\n",
    "\n",
    "    # Dataloaders\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_data, params[\"batch_size\"], num_workers=params[\"num_workers\"]\n",
    "    )\n",
    "    # validation_dataloader = torch.utils.data.DataLoader(validation_data, params['batch_size'],num_workers=params['num_workers'])\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_data, params[\"batch_size\"], num_workers=params[\"num_workers\"]\n",
    "    )\n",
    "\n",
    "def access(train_dataloader):\n",
    "    for X,Y in train_dataloader:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "552c9644091b0c8a03658003f6bc4e6be186a8b6576a345edbde5d33b7af3828"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
