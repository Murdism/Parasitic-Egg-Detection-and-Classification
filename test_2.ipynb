{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from read_dataset import build_df\n",
    "from utils import CFG2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from models import *\n",
    "from main import *\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datapreprocess import *\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_FILES = CFG2.img_path \n",
    "LABEL_FILES = CFG2.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(IMG_FILES,LABEL_FILES):\n",
    "    # x_train, x_valid, x_test, y_train, y_valid, y_test\n",
    "    dir_names = ['train','valid','test']\n",
    "    # images\n",
    "    x_train, x_valid, x_test, y_train, y_valid, y_test=[],[],[],[],[],[]\n",
    "    for dir in (os.listdir(IMG_FILES)):\n",
    "        if dir in dir_names:\n",
    "            for img_name in (os.listdir(os.path.join(IMG_FILES, dir))):\n",
    "                img_path = os.path.join(IMG_FILES, dir, img_name)\n",
    "\n",
    "                if dir  == 'train':\n",
    "                    x_train.append(img_path)\n",
    "\n",
    "                elif dir == 'valid':\n",
    "                    x_valid.append(img_path)\n",
    "\n",
    "                elif dir  == 'test':\n",
    "                    x_test .append(img_path)\n",
    "    \n",
    "    for l_dir in (os.listdir(LABEL_FILES)):\n",
    "        if l_dir in dir_names:\n",
    "            for label in (os.listdir(os.path.join(LABEL_FILES, l_dir))):\n",
    "                label_path = os.path.join(LABEL_FILES, l_dir,label)\n",
    "\n",
    "                if l_dir  == 'train':\n",
    "                    y_train.append(label_path)\n",
    "\n",
    "                elif l_dir == 'valid':\n",
    "                    y_valid.append(label_path)\n",
    "\n",
    "                elif l_dir  == 'test':\n",
    "                    y_test .append(label_path)\n",
    "\n",
    "    train_data = [x_train,y_train]\n",
    "    valid_data = [x_valid,y_valid]\n",
    "    test_data = [x_test,y_test]\n",
    "    return np.array(train_data), np.array(valid_data), np.array(test_data)\n",
    "\n",
    "    # class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data= reader(IMG_FILES, LABEL_FILES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cell_datatset/labels/train/Opisthorchis viverrine_0938.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=True,IMAGE_SIZE = 640):\n",
    "        super(Custom_Dataset, self).__init__()\n",
    "        # List of files\n",
    "        self.IMAGE_SIZE = IMAGE_SIZE\n",
    "        self.data_files = data[0]  # [DATA_FOLDER.format(id) for id in ids]\n",
    "        self.label_files = data[1]  # [LABELS_FOLDER.format(id) for id in ids]\n",
    "        self.class_labels = torch.LongTensor(self.get_class_labels()) \n",
    "        self.transform = transform\n",
    "        # Sanity check : raise an error if some files do not exist\n",
    "        for f in self.data_files:\n",
    "            if not os.path.isfile(f):\n",
    "                raise KeyError(\"{} is not a file !\".format(f))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)  # the length of the used data\n",
    "    \n",
    "    def img_size(self,idx):\n",
    "        self.img =np.asarray(io.imread(self.data_files[idx]))\n",
    "        return (self.img).shape\n",
    "    \n",
    "    def get_class_labels(self):\n",
    "        labels  = []\n",
    "        for fname in self.label_files:\n",
    "            with open(fname, \"r\") as file:\n",
    "                labels.append(file.read().split(\" \")[0])\n",
    "        return np.array(labels,dtype=np.uint8)\n",
    "\n",
    "    def original_img(self,idx):\n",
    "        self.img =(\n",
    "                   1/255\n",
    "             *np.asarray(\n",
    "                io.imread(self.data_files[idx], plugin=\"pil\").transpose(\n",
    "                    (2, 0, 1)\n",
    "                ),\n",
    "                dtype=\"float32\",\n",
    "            )\n",
    "        )\n",
    "        return self.img,self.labels[idx]\n",
    "\n",
    "    def get_class_label(self,idx):\n",
    "        return self.class_labels[idx]\n",
    "\n",
    "    def resized_bbox(self,idx):\n",
    "        image = Chitra(self.data_files[idx], self.labels[idx], self.class_labels[idx])\n",
    "        # Chitra can rescale your bounding box automatically based on the new image size.\n",
    "        image.resize_image_with_bbox((640, 640))\n",
    "\n",
    "        return np.array([image.bboxes[0].x1_int,image.bboxes[0].y1_int,image.bboxes[0].x2_int,image.bboxes[0].y2_int])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #         Pre-processing steps\n",
    "        \n",
    "        if self.transform is not None:\n",
    "\n",
    "            self.data = (\n",
    "                np.asarray(\n",
    "                io.imread(self.data_files[idx], plugin=\"pil\").transpose(\n",
    "                    (2, 0, 1)\n",
    "                ),\n",
    "                dtype=\"float32\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "            # convert the tensor to image\n",
    "            tensor2image_transform = transforms.ToPILImage()\n",
    "\n",
    "            # make the format avialble to resnet model\n",
    "            data_p = torch.tensor(\n",
    "                self.data \n",
    "            )  # tranform the image into ResNet format\n",
    "\n",
    "            image = tensor2image_transform(data_p)\n",
    "            data_p = self.transform(\n",
    "                image\n",
    "            )  # transform the image into ResNet format\n",
    "            return data_p, self.class_labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"gets gpu for mac m1 or cuda, or cpu machine\"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print('Running on Cuda GPU')\n",
    "        return device\n",
    "        # x = torch.ones(1, device=mps_device)\n",
    "        # print(x)\n",
    "        \n",
    "    \n",
    "    elif torch.backends.mps.is_available():\n",
    "        print('Running on the Mac GPU')\n",
    "        mps_device = torch.device(\"mps\")\n",
    "        return mps_device\n",
    "        \n",
    "    else:\n",
    "        # print(\"MPS device not found.\")\n",
    "        return torch.device('cpu')\n",
    "        print('Code Running on a CPU')\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_history(training_accs, training_losses):# Get training and test loss histories\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_accs) + 1)\n",
    "\n",
    "    # Visualize loss history\n",
    "    plt.plot(epoch_count, training_losses, 'r--')\n",
    "    plt.plot(epoch_count, training_accs, 'b-')\n",
    "    plt.legend(['Training Loss', 'Training Accuracy'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy and Loss Curves')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# testing/ validation of unet model\n",
    "def validate_model(trained_model, test_dataloader, device=None):\n",
    "    trained_model.eval() # switch off some layers such as dropout layer during validation\n",
    "    with torch.no_grad():\n",
    "        test_predictions = []\n",
    "        test_labels = []\n",
    "\n",
    "        for i, (x, y) in enumerate(test_dataloader):\n",
    "            x  = x.to(device)\n",
    "            y  = y.to(device)\n",
    "            y_hat = trained_model(x)\n",
    "            predicted_labels =  torch.argmax(y_hat, dim=1).detach().cpu().numpy().tolist()\n",
    "            # y_hat  = y_hat.cpu()\n",
    "            y = y.detach().cpu().numpy().tolist()\n",
    "            test_predictions.extend(predicted_labels)\n",
    "            test_labels.extend(y)\n",
    "    \n",
    "        return test_labels, test_predictions\n",
    "\n",
    "\n",
    "def evalution_metrics(ground_truth, predictions):\n",
    "    print(f\"mean acc score = {accuracy_score(ground_truth, predictions)}\")\n",
    "    print(f\"mean recall score = {recall_score(ground_truth, predictions, average='micro')}\")\n",
    "    print(f\"precision score = {precision_score(ground_truth, predictions, average='micro')}\")\n",
    "    print(f\"mean f1 score = {f1_score(ground_truth, predictions, average='micro')}\")\n",
    "    labels = np.unique(ground_truth).tolist()\n",
    "    cm  = confusion_matrix(ground_truth, predictions, labels=labels) \n",
    "    report  = classification_report(ground_truth, predictions)\n",
    "    print(report)\n",
    "\n",
    "    sns.heatmap(cm)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = preprocess_image()\n",
    "train_dataset  = Custom_Dataset(train_data, transform=transform)\n",
    "valid_dataset = Custom_Dataset(valid_data, transform=transform)\n",
    "test_dataset = Custom_Dataset(test_data, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Cuda GPU\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_default_device()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "EPOCHS = 20\n",
    "# Parameters\n",
    "params = {\"batch_size\": 4, \"shuffle\": True, \"num_workers\": 4}\n",
    "NUM_CLASSES = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, params[\"batch_size\"], num_workers=params[\"num_workers\"], shuffle=params[\"shuffle\"],\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(valid_dataset, params['batch_size'],num_workers=params['num_workers'])\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, params[\"batch_size\"], num_workers=params[\"num_workers\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, filters=32, num_classes=11):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1  = nn.Conv2d(input_channels, filters, kernel_size=2) \n",
    "        self.conv2  = nn.Conv2d(filters, filters * 2, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(filters * 2, filters * 4, kernel_size=3)\n",
    "\n",
    "        self.fc1  =  nn.Linear(in_features=  (filters * 4 * 28*28  + 1_000) , out_features=64)\n",
    "        self.out = nn.Linear(64, out_features=num_classes)\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "    def forward(self, x, x2):\n",
    "        x  = self.conv1(x) # output = (B, C,224,224)\n",
    "        x  = F.relu(x) \n",
    "        x  = F.max_pool2d(x, kernel_size=2, padding=1) #output = (B,C,112,112)\n",
    "        x  = F.dropout(x, 0.2)\n",
    "\n",
    "        x  = self.conv2(x) #output = (B,iC,112,112)\n",
    "        x  = F.relu(x)\n",
    "        x  = F.max_pool2d(x, kernel_size=2, padding=1) # #output = (B,iC,56,56)\n",
    "        x  = F.dropout(x, 0.2)\n",
    "        x  = self.conv3(x)\n",
    "        x  = F.relu(x)\n",
    "        x  = F.max_pool2d(x, kernel_size=2, padding=1) # output = (B, iC, 28 , 28)\n",
    "        x  = F.dropout(x, 0.2)\n",
    "        x  = self.flatten(x)\n",
    "        # print(\"x2 size = \", x2.size())\n",
    "        # print(\n",
    "        #     'size of x  = ', x.size(),\n",
    "        # )\n",
    "        x  = torch.cat((x, x2), dim=1)\n",
    "        # print(\"cat size = \", x.size())\n",
    "        x  = self.fc1(x)\n",
    "        x  = F.relu(x)\n",
    "        x  = F.dropout(x, 0.25)\n",
    "        x  = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_model():\n",
    "    \n",
    "    resnet152 = torchvision.models.resnet152(pretrained=True)\n",
    "    resnet152.eval()\n",
    "    # remove the last layer\n",
    "    # new_model = torch.nn.Sequential(*(list(resnet152.children())[:-1]))\n",
    "    # freeze the model\n",
    "    for param in resnet152.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # if device is not None:\n",
    "    #     resnet152.to(device)\n",
    "\n",
    "    return resnet152\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trainCustomModel(resnetModel,yoloDetectModel,customCNNModel, train_dataloader, optimizer,loss_fn,in_channels=3,num_classes=11, epochs=30, learning_rate=0.001, device='cpu'):\n",
    "    \"\"\"Accepts feature from resnet \n",
    "    and yolo object detection cropped iamge(s) \n",
    "    as features to train an accurate cnn classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    training_losses = []\n",
    "    training_accs = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        number_of_batches = 0\n",
    "        epoch_loss_values = 0.0\n",
    "        epoch_accs = 0.0\n",
    "        for index, train_batch in enumerate(tqdm(train_dataloader)):\n",
    "            # X,  y = X.to(device), y.to(device)  # make the tensors in gpu state\n",
    "            X   = train_batch[0]\n",
    "            y = train_batch[1]\n",
    "            X  = Variable(X, requires_grad=True).to(device)\n",
    "            y  = torch.LongTensor(y).to(device)\n",
    "\n",
    "            # X_valid  = Variable(valid_batch[0], requires_grad=False).to(device)\n",
    "            # y_valid = torch.LongTensor(valid_batch[1]).to(device)\n",
    "\n",
    "            # resnet processing\n",
    "            # resnet_X = Variable(resnet_preprocess((X.cpu())), requires_grad=True).to(device)\n",
    "            print('Before Resnt')\n",
    "            resnet_X  = resnetModel(X)\n",
    "            print('After Resnt')\n",
    "\n",
    "            # extracted_features = resnetModel(resnet_X) # (Bacth, 1000) tensor is returned\n",
    "            # X2  = Variable(torch.randn(size= (X.size()[0],1000)), requires_grad=True).to(device)\n",
    "            preds = customCNNModel(X, resnet_X)\n",
    "            print('After customCNNModel')\n",
    "        \n",
    "\n",
    "            loss = loss_fn(preds, y).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            probs = torch.log_softmax(preds, dim=1)\n",
    "            predicted_labels = torch.argmax(probs, dim=1)\n",
    "\n",
    "            # acc\n",
    "            epoch_accs += accuracy_score(y.detach().cpu(),predicted_labels.detach().cpu())\n",
    "            epoch_loss_values += loss.item()\n",
    "\n",
    "            number_of_batches += 1\n",
    "\n",
    "        # compute average of batch loss and accuracy\n",
    "        batch_acc, batch_loss = epoch_accs / \\\n",
    "            number_of_batches, epoch_loss_values / number_of_batches\n",
    "        training_losses.append(batch_loss)\n",
    "        training_accs.append(batch_acc)\n",
    "\n",
    "        print(\"Epoch:{}/{}, acc={:.3f}%, loss={:.3f}\".format(epoch,epochs, batch_acc*100, batch_loss))\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    return training_accs, training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CHANNELS  = 3\n",
    "NUM_FEATURES  = 16\n",
    "customCNNModel = CNN(INPUT_CHANNELS, NUM_FEATURES, NUM_CLASSES).to(DEVICE) \n",
    "optimizer = torch.optim.Adam(\n",
    "        params=customCNNModel.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnetModel  = get_pretrained_model().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1965/1965 [04:59<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/50, acc=8.982%, loss=2.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 605/1965 [01:31<03:25,  6.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_accs, training_losses \u001b[39m=\u001b[39m trainCustomModel(resnetModel, \u001b[39mNone\u001b[39;00m,customCNNModel, train_dataloader, optimizer,loss_fn, in_channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,num_classes\u001b[39m=\u001b[39m\u001b[39m11\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, device\u001b[39m=\u001b[39mDEVICE)\n",
      "Cell \u001b[0;32mIn [12], line 47\u001b[0m, in \u001b[0;36mtrainCustomModel\u001b[0;34m(resnetModel, yoloDetectModel, customCNNModel, train_dataloader, optimizer, loss_fn, in_channels, num_classes, epochs, learning_rate, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m y  \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(y)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     42\u001b[0m \u001b[39m# X_valid  = Variable(valid_batch[0], requires_grad=False).to(device)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# y_valid = torch.LongTensor(valid_batch[1]).to(device)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[39m# resnet processing\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# resnet_X = Variable(resnet_preprocess((X.cpu())), requires_grad=True).to(device)\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m resnet_X  \u001b[39m=\u001b[39m resnetModel(X)\n\u001b[1;32m     49\u001b[0m \u001b[39m# extracted_features = resnetModel(resnet_X) # (Bacth, 1000) tensor is returned\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m# X2  = Variable(torch.randn(size= (X.size()[0],1000)), requires_grad=True).to(device)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m preds \u001b[39m=\u001b[39m customCNNModel(X, resnet_X)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco_py/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco_py/lib/python3.8/site-packages/torchvision/models/resnet.py:220\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 220\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco_py/lib/python3.8/site-packages/torchvision/models/resnet.py:210\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    208\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[1;32m    209\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 210\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer3(x)\n\u001b[1;32m    211\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n\u001b[1;32m    213\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco_py/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco_py/lib/python3.8/site-packages/torch/nn/modules/container.py:117\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    116\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco_py/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco_py/lib/python3.8/site-packages/torchvision/models/resnet.py:104\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m--> 104\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    105\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m    106\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco_py/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco_py/lib/python3.8/site-packages/torch/nn/modules/conv.py:423\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 423\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco_py/lib/python3.8/site-packages/torch/nn/modules/conv.py:419\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    416\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    417\u001b[0m                     weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    418\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 419\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    420\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_accs, training_losses = trainCustomModel(resnetModel, None,customCNNModel, train_dataloader, optimizer,loss_fn, in_channels=3,num_classes=11, epochs=50, learning_rate=0.001, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(training_accs, training_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('mujoco_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2f424a28026550c59ba80030c3368e53b944a64380b7979f5e4c49b470b0842"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
